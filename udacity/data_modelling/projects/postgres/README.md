# Sparkifydb


Sparkify is an imaginary music streaming startup seeking to develop a database model that makes querying data from users song plays easy. This document shows how this database can be built and which files to run for immediate use.


- [Requirements](#requirements)
- [Quick use](#quick-use)
- [The database](#the-database)
- [Database schema design](#database-schema-design)
- [Tables and data types](#tables-and-data-types)
  * [Fact table](#fact-table)
  * [Dimension tables](#dimension-tables)
- [ETL pipeline](#etl-pipeline)
- [Query example](#query-example)

## Requirements

- Python 3.x.
- psycopg2.
- pandas.
- postgreSQL installed and configured.

## Quick use

To create the database and insert records from the datasets, run in terminal:

1. ```python create_tables.py```

2. ```python etl.py```


## The database

Two datasets feed records to the database: the first dataset is a subset from the well known [Million Song Dataset](http://millionsongdataset.com/), that gathers a bunch of json files about songs. In the second there are log files, also in JSON format, generated by [an event simulator](https://github.com/Interana/eventsim) based on the songs in the first dataset.

Five tables are used to build the database, that will be named after sparkifydb from now on: ```songplays```, ```users```, ```songs```, ```artists``` and ```time```. Details about the schema are explained right up next.


## Database schema design

A [star schema](https://en.wikipedia.org/wiki/Star_schema) is used to split sparkifydb in fact and dimension tables. The songplays table is the fact table and the other tables are the dimension tables. In that configuration foreign keys aren't really necessary, it's easy to explain how the schema works to someone interested in the data and queries will be faster.

Data types were chosen based on data from the files, for example: usually someone would pick ```int``` for a primary key like ```artist_id```, but ```data/song_data``` files show that it would make more sense for ```artist_id``` to have ```text``` or ```varchar``` as a data type. If you want to take a look at this, run the following command in your terminal:

```cat data/song_data/A/B/C/TRABCEI128F424C983.json | python -m json.tool```

To create or drop tables, run in terminal:

```python create_tables.py```


The create/drop table statements can be found in ```sql_queries.py```. For more details about data types check below.


## Tables and data types


* "pk" stands for primary key

### Fact table

```songplays``` - songplay_id (auto incremented pk), start_time text, user_id text, level text, song_id text, artist_id text, session_id int, location text, user_agent text.

### Dimension tables

```users``` - user_id text (pk) , first_name text, last_name text, gender varchar(1), level text.

```songs``` - song_id text (pk), title text, artist_id text, year int, duration float.

```artists``` - artist_id text (pk) , name text, location text, latitude float, longitude float.

```time``` - start_time text (pk), hour int, day int, week int, month int, year int, weekday text.


## ETL pipeline

The ETL pipeline consists in inserting data from files in each table of the database(sparkifydb) in order. It starts inserting data from ```data/song_data``` into ```songs``` and ```artists``` tables, and after that inserts filtered data from ```data/log_data``` into the remaining tables.

Tables ```songplays```, ```users``` and ```time``` only receive data where ```page``` keys from ```data/log_data``` files equals to ```"NextSong"```. Records added to ```time``` table are made by splitting the timestamp obtained in ```data/log_data``` into the columns specification given by the schema above. Two columns, ```artistid``` and ```songid``` from ```Songplays```, are obtained by combining data from ```artists``` and ```songs```, where a select statement on ```sql_queries.py``` does the job of finding the ids.


The insert statements are in ```sql_queries.py``` and some of them were written with constraints to avoid duplicate data from halting the pipeline flow. Here is an example:

```
artist_table_insert = "INSERT INTO artists(artist_id, name, location, \
latitude, longitude) \
VALUES(%s, %s, %s, %s, %s) ON CONFLICT (artist_id) DO NOTHING"
```

To execute the ETL pipeline, type the command below in the terminal(but remember to check if create_tables.py was already executed):

```
python etl.py
```

You will see the entire process being executed, and if something goes wrong an elucidative message will be displayed on screen. For a more detailed guide on the ETL pipeline, check ```etl.ipynb```.

## Query example

You can try some queries out by yourself using the ```test.ipynb```. After running the first two blocks of code, if no errors have occurred, it's all set. Here is an example: you can search for songs and artists based on year, in this case 80's songs were selected.

Query:

```
%sql SELECT a.title, b.name FROM songs a \
JOIN artists b \
ON b.artist_id = a.artist_id \
WHERE a.year BETWEEN 1980 AND 1989;
```

Result:

|                          title |             name |
|-------------------------------:|-----------------:|
|                     Pink World | Planet P Project |
|                Something Girls |         Adam Ant |
|                       Floating |       Blue Rodeo |
|        Tonight Will Be Alright |    Lionel Richie |
| James (Hold The Ladder Steady) |     SUE THOMPSON |




#### Warning


This project is part of Udacity's Nanodegree in Data Engineering. Some parts of the text do take significant inspiration on the project overview and its instructions.
